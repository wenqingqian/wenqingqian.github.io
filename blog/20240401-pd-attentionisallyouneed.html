

<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Blog</title>
	<link rel="icon" href="../icon/appletree.png" type="image/png">
	<link href="../render_base/fonts.css" rel="stylesheet">
	<link href="../render_base/blog.css" rel="stylesheet">
	<link href="../render_base/style_base.css" rel="stylesheet">
	<link href="../render_base/style_chinese.css" rel="stylesheet">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Ma+Shan+Zheng&family=ZCOOL+XiaoWei&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
	<link href="../render_base/sidebar.css" rel="stylesheet">
	<link href="../render_base/common.css" rel="stylesheet">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<main>
<div class='disp-toc-header'>
<h1>[Transformer] Attention is all you need</h1><h2 style="font-family: 'ZCOOL XiaoWei', regular;"> 2024-4-1</h2>
</div><p><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">论文原文</a>, <a href="https://www.bilibili.com/video/BV1ZG411y7aZ/?spm_id_from=333.788.top_right_bar_window_history.content.click&vd_source=6823fff4176fbb99da705bde30eb8cea">本文引用</a></p>
<section>
<h1>intro</h1>
<p>原始的循环神经网络无法处理长距离依赖问题, LSTM 可以改善这一点但做的不够好, 并且它们的网络结构决定了无法进行并行训练</p>
<p>本文提出 Transformer, 完全通过 attention 来处理全局依赖</p>
</section>
<section>
<h1>process</h1>
<img src="../generator/raw_file/blog_markdown/./pic/llmpost/attentionisallyouneed/transformer.png" style="width: 30%;" alt="err! email me if you need">
<img src="../generator/raw_file/blog_markdown/./pic/llmpost/attentionisallyouneed/transformer-attention.png" style="width: 30%;" alt="err! email me if you need">
<img src="../generator/raw_file/blog_markdown/./pic/llmpost/attentionisallyouneed/multiheadattention.png" style="width: 30%;" alt="err! email me if you need">
<p>如上图, 一个 Transformer 可以被分为 encoder(左)和 decoder(右), 假设编码器接受一串文本</p>
<ul>
<li>首先输入会被分解为一段数字, 每个数字代表一个词</li>
<li>然后进入嵌入层, 每个表示词的数字会被提升为一个向量, 即用一个向量表示一个词, 向量可以存储更多信息(如空间距离)</li>
<li>接着进行位置编码, 将每个词的位置信息与之前的词向量相加</li>
<li>接着进入 MHA, 假设输入为 X, 则\(Q=XW^Q,K=XW^K,V=XW^V\)</li>
<li>...</li>
</ul>
<p>具体计算过程</p>
<p>Positional Encoding:</p>
<div>
\[
\begin{aligned}
PE_{(pos,2i)}&=sin(pos/10000^{2i/d_{model}})\\
PE_{(pos,2i+1)}&=cos(pos/10000^{2i/d_{model}})
\end{aligned}
\]
</div>
<p>Multi-Head Attention:</p>
<div>
\[
\begin{aligned}
	MultiHead(Q,K,V)&=Concat(head_1,\dots,head_h)W^O\\
	where\space head_i&=Attention(QW_i^Q,KW_i^K,VW_i^V)
\end{aligned}
\]
</div>
<p>多头注意力机制, 每一个注意力通过不同的权重矩阵关注不同的信息, 每个注意力可以并行计算</p>
<p>Scaled Dot-Product Attention:</p>
<div>
\[
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\]
</div>
<p>等效为\(softmax(XX^T)X\)</p>
<ul>
<li>对于\(XX^T\), 将X视为一些行向量(词向量)组成的矩阵, 它乘上自己的转置相当于不同词的内积, 即两个向量之间的夹角或者说投影面积, 这个值越大可以理解为相关度越高, 注意力机制要做的就是根据这个相关度给予不同的权重</li>
<li>接着通过softmax将一个词与其他各个词的内积转换为概率, 然后用概率乘上原始矩阵就是新的词向量, 是原词向量经过注意力机制加权求和后的结果</li>
</ul>
<img src="../generator/raw_file/blog_markdown/./pic/llmpost/attentionisallyouneed/vector.png" style="display: block; margin: 0 auto;width: 90%;" alt="err! email me if you need">
<figure><figcaption>Attention-Visualization</figcaption></figure><p>图中的彩色线条就是对应的 softmax 后的概率(权重), softmax 用于归一化, 原词向量等于原矩阵各词向量加权求和的结果, 权重来源向量内积, 与上述数学公式一一对应, QKV 就是在 X 基础上引入可训练参数, 提高模型拟合能力</p>
<p>Positon-wise Feed-Forward Networks:</p>
<div>
\[
FFN(x)=max(0,xW_1+b_1)W_2+b_2
\]
</div>
<p>两个线性变换夹一个 ReLU</p>
<p>解码器通过掩码保证只关注前面已经生成的文本</p>
</section>

	
	<div class="settings-menu">
		<button class="settings-button">Control</button>
		<div class="settings-options">
			<a class="settings-button" href="../index.html">home</a>
			<button id="color_mode_button" class="settings-button" onclick="toggleDarkMode()"> dark</button>
			<a style="color: var(--basic-black);">width <button id="adjustWidth-add" class="settings-inner-button">+</button> / <button id="adjustWidth-sub" class="settings-inner-button">-</button></a>
			<a class="settings-button" href="../blog.html">back to blog</a>
		</div>
	</div>
</main>


<script src="../render_base/sidebar.js"></script> 
<script src="../render_base/common.js"></script> 

</body>
</html>

