

<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Blog</title>
	<link rel="icon" href="../icon/appletree.png" type="image/png">
	<link href="../render_base/fonts.css" rel="stylesheet">
	<link href="../render_base/style_base.css" rel="stylesheet">
	<link href="../render_base/blog.css" rel="stylesheet">
	<link href="../render_base/style_english.css" rel="stylesheet">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Ma+Shan+Zheng&family=ZCOOL+XiaoWei&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
	<link href="../render_base/sidebar.css" rel="stylesheet">
	<link href="../render_base/common.css" rel="stylesheet">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<main>
<div class='disp-toc-header'>
<h1>QLoRA: Efficient Finetuning of Quantized LLMs</h1><h2 style="font-family: 'ZCOOL XiaoWei', regular;"> 2024-4-12</h2>
</div><p><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf">original paper</a></p>
<section>
<h1>intro</h1>
<p>QLoRA introduces a number of innovations to save memory without sacrificing performance:</p>
<ul>
<li>4-bit NormalFloat (NF4)</li>
<li>Double Quantization</li>
<li>Paged Optmizers</li>
</ul>
<img src="../generator/raw_file/blog_markdown/./pic/llmpost/qlora/diff.png" style="display: block; margin: 0 auto;width: 70%;" alt="err! email me if you need">
</section>
<section>
<h1>Approach</h1>
<h2>Block-wise k-bit Quantization</h2>
<p>Quantizing FP32 to Int8 with range [-127,127]</p>
<div>
\[
X^{Int8}=round\left(\frac{127}{absmax(X^{FP32})}X^{FP32}\right)=round(c^{FP32}\cdot X^{FP32})
\]
</div>
<div>
\[
dequant(c^{FP32},X^{Int8})=\frac{X^{Int8}}{c^{FP32}}=X^{FP32}
\]
</div>
<p>Chunking the input into independently quantized blocks with their own \(c\) to reduce the influence of large magnitude value which will lead the other value to be too small</p>
<h2>Quantile quantization</h2>
<p>For k-bit quantile quantization:</p>
<ul>
<li>sort the data</li>
<li>divide data into \(2^k\) equal parts</li>
<li>each data have its own index \(i\in [0,2^k-1]\)</li>
</ul>
<p>store the data with this index, and recovered by follow function in calculation:</p>
<div>
\[
q_{i} = \frac{1}{2}\left(Q_X\left(\frac{i}{2^k+1}\right)+Q_X\left(\frac{i+1}{2^k+1}\right)\right)
\]
</div>
<p>while \(Q_X(\cdot)\) is the quantile function of the empirical cumulative distribution, i.e., \(Q_X(0.5)\) means the median of distribution</p>
<h2>NormalFloat</h2>
<p>while the process of quantile estimation which include sort and divide is expensive, actually, the pre-trained nn weights usually have a zero-centered normal distribution with standard deviation \(\sigma:N(0,\sigma^2)\)</p>
<p>assuming \(\sigma = 1\), in this case:</p>
<ul>
<li>estimate the \(2^k+1\) quantiles of \(N(0,1)\) distribution through equation (3)</li>
<li>normalize these \(2^k+1\) quantiles into [-1,1]</li>
<li>Finally directly quantize the input by normalizing it into the [-1,1] range through absolute maximum rescaling</li>
</ul>
<p>In other words:</p>
<ul>
<li>step 1: we can get the certain quantiles without the data but through the distribution</li>
<li>step 2: we can get the normalized certain quantiles</li>
<li>step 3: after this step, input and quantiles have the same scale that we can easily get the index \(i\) through comparison</li>
</ul>
<p>we only need do step 3 each time</p>
<p>A problem is no exact representation of 0, which may have the special means</p>
<p>TODO(something not understand):</p>
<ul>
<li>why estimate \(2^k+1\) quantiles, \(2^k-1\) are enough to divide data into \(2^k\) parts</li>
<li>the equation (3), what does \(i\) mean and its range</li>
<li>zero representation method</li>
</ul>
<h2>Double Quantization</h2>
<p>In Block-wise k-bit Quantization, each block have a quantization constant \(c^{FP32}\), assum block size is \(b_1\)</p>
<div>
\[
bits\space per\space parameter=k+(\frac{32}{b_1})
\]
</div>
<p>we can quantize the \(\)c^{FP32}\(\) to \(\)c_2^{FP8}\(\), by chunking the \(\)c^{FP32}\(\) with block size \(\)b_2\(\)</p>
<div>
\[
bits\space per\space parameter=k+\frac{8}{b_1}+\frac{32}{b_1*b_2}
\]
</div>
<h2>Paged Optimizers</h2>
<p>Transfer the optimizer states from GPU to CPU RAM when GPU runs out-of-memory and paged back into GPU when it is needed</p>
</section>

	
	<div class="settings-menu">
		<button class="settings-button">Control</button>
		<div class="settings-options">
			<a class="settings-button" href="../index.html">home</a>
			<button id="color_mode_button" class="settings-button" onclick="toggleDarkMode()"> dark</button>
			<a style="color: var(--basic-black);">width <button id="adjustWidth-add" class="settings-inner-button">+</button> / <button id="adjustWidth-sub" class="settings-inner-button">-</button></a>
			<a class="settings-button" href="../blog.html">back to blog</a>
		</div>
	</div>
</main>


<script src="../render_base/sidebar.js"></script> 
<script src="../render_base/common.js"></script> 

</body>
</html>

