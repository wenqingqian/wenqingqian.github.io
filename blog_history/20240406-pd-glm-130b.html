

<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Blog</title>
	<link rel="icon" href="../icon/appletree.png" type="image/png">
	<link href="../render_base/fonts.css" rel="stylesheet">
	<link href="../render_base/blog.css" rel="stylesheet">
	<link href="../render_base/style_base.css" rel="stylesheet">
	<link href="../render_base/style_chinese.css" rel="stylesheet">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Ma+Shan+Zheng&family=ZCOOL+XiaoWei&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
	<link href="../render_base/sidebar.css" rel="stylesheet">
	<link href="../render_base/common.css" rel="stylesheet">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<main>
<div class='disp-toc-header'>
<h1>GLM-130B: An Open Bilingual Pre-trained Model</h1><h2 style="font-family: 'ZCOOL XiaoWei', regular;"> 2024-4-6</h2>
</div><p><a href="https://arxiv.org/pdf/2210.02414.pdf">论文原文</a></p>
<section>
<h1>intro</h1>
<p>本文将介绍 GLM-130B, 一个有着 130 billion 参数的双语预训练模型, 并分享这样规模的模型的设计思路和训练策略</p>
</section>
<section>
<h1>design choice</h1>
<p>大部分近期(5 Oct 2022)的100B-scale LLMs 比如GPT-3, OPT, PaLM, BLOOM都使用传统的GPT风格, 仅解码器自回归语言模型</p>
<p>GLM-130B 使用 GLM 作为骨干网络, RoPE代替ALiBi作为位置编码, GeLU作为激活函数, 使用DeepNorm类似的Post-LN增强训练稳定性</p>
</section>
<section>
<h1>pretrain</h1>
<p>GLM-130B 的预训练目标不仅包括自监督的GLM空白段自回归, 还有小部分的多任务学习以提高在下游 zero-shot 情况下的表现</p>
<ul>
<li>Self-Supervised Blank Infilling(95% tokens)</li>
<li>Multi-Task Instruction Pre-Training(MIP, 5% tokens)</li>
<p style="margin-bottom: 0em;">T5和ExT5指出预训练多任务学习可能比微调更有效, 并且在预训练阶段进行有助于避免破坏LLMs的通用能力</p>
<p style="margin-bottom: 0em;">建议在GLM-130B的预训练中包括各种指令提示的数据集, 包括语言理解, 生成和信息提取</p>
</ul>
</section>
<section>
<h1>parallel strategies</h1>
<p>GLM-130B使用 40G A100s, 为了处理巨大的GPU内存需求和应用张量并行导致的整体GPU利用率降低, 我们将流水线并行与数据并行, 张量模型并行相结合得到新的三维并行策略</p>
<p>具体的, 流水线将模型分为几个并行组, 为了最小化流水线引入的bubbles, 使用 PipeDream-Flush来训练具有相对较大全局批次大小(4224)的GLM-130B, 以减少时间和GPU内存浪费. 通过数值和经验检验, 采用了4段张量并行和8段流水线并行</p>
</section>
<section>
<h1>training stability</h1>
<p>考虑到计算资源限制, 必须在不同浮点(FP)格式的效率和稳定性之间做出权衡, 低精度浮点数比如 FP16 能提高计算效率, 但是容易出现上溢和下溢错误, 导致训练崩溃</p>
<p>GLM-130B 使用常见的混合精度: FP16 用于前向反向, FP32 用于优化器和主权重</p>
<p>与 OPT-175B 和 BLOOM-176B 相似, 这种选择导致 GLM-130B 也会面临频繁的损失尖峰, 其中: </p>
<ul>
<li>OPT-175B 试图通过手动跳过数据和调整超参数来解决</li>
<li>BLOOM-176B选择 embedding norm</li>
</ul>
<p>GLM-130B 使用 Embedding Layer Gradient Shrink(EGS), 经验表明, 梯度范数可以作为训练崩溃的信息指标:</p>
<ul>
<li>训练崩溃通常滞后于梯度范数尖峰几步</li>
<li>这种尖峰通常是由嵌入层的异常梯度引起的</li>
</ul>
<img src="../generator/raw_file/blog_markdown/./pic/llmpost/glm130b/egs1.png" style="width: 38%;" alt="err! email me if you need">
<img src="../generator/raw_file/blog_markdown/./pic/llmpost/glm130b/egs2.png" style="width: 38%;" alt="err! email me if you need">
<figure><figcaption>EGS reduces gradient scale and variance to stabilize LLMs' pre-training</figcaption></figure><p>嵌入层上的梯度收缩可以克服损失尖峰, 从而稳定GLM-130B的训练</p>
</section>
<section>
<h1>transformer scale up</h1>
<p>在使用大量transformer时会出现以下问题:</p>
<ul>
<li>如果使用 Pre-LN, transformer 主分支的值在深层会极大</li>
<p style="margin-bottom: 0em;">GLM-130B通过使用基于DeepNorm的Post-LN, 使得值总是有界的</p>
<li>随着模型的扩大, 注意力得分会超过FP16的范围</li>
<p style="margin-bottom: 0em;">PB-Relax 通过在注意力计算中删除偏差项并扣除极值以避免该问题, 但这无助于避免GLM-130B中的不收敛</p>
<p style="margin-bottom: 0em;">BLOOM-176B中, 由于其在NVIDIA Ampere GPU(即A100)上的值范围很广, 因此使用BF16而不是FP16. 然而, BF16比FP16多消耗约15%的运行时GPU内存, 因为它在梯度累积中转换为FP32, 并且它在其他GPU平台上不受支持</p>
<p style="margin-bottom: 0em;">BLOOM-176B的另一个选项是将embedding norm应用于BF16, 但embedding norm会损害模型的zero-shot学习</p>
</ul>
</section>
<section>
<h1>GLM-130B Inference on RTX 2080Ti</h1>
<p>GLM-130B的一个目标就是在没有效率和效果损失的情况下降低使用100B规模LLM的硬件需求</p>
<h2>INT4 Quantization for RTX 3090s/2080s</h2>
<p>实践是将模型权重和激活都量化为INT8</p>
<p>LLM的激活可能包含极端异常值</p>
<ul>
<li>OPT-175B和BLOOM-176B中异常值仅影响约0.1%的特征维度, 因此通过对异常维度的矩阵乘法分解来解决</li>
<li>GLM-130B的激活中存在大约30%的异常值, 这使得上述技术的效率要低得多</li>
</ul>
<p>GLM-130B将模型权重(大部分是线性层)进行量化(成功达到INT4), 并保持激活层的FP16, 量化模型在运行时会动态转换成FP16, 这引入了较小的计算开销, 但大大减少了存储模型权重所需GPU内存</p>
</section>

	
	<div class="settings-menu">
		<button class="settings-button">Control</button>
		<div class="settings-options">
			<a class="settings-button" href="../index.html">home</a>
			<button id="color_mode_button" class="settings-button" onclick="toggleDarkMode()"> dark</button>
			<a style="color: var(--basic-black);">width <button id="adjustWidth-add" class="settings-inner-button">+</button> / <button id="adjustWidth-sub" class="settings-inner-button">-</button></a>
			<a class="settings-button" href="../blog.html">back to blog</a>
		</div>
	</div>
</main>


<script src="../render_base/sidebar.js"></script> 
<script src="../render_base/common.js"></script> 

</body>
</html>

