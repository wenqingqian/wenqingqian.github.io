

<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Blog</title>
	<link rel="icon" href="../icon/appletree.png" type="image/png">
	<link href="../render_base/fonts.css" rel="stylesheet">
	<link href="../render_base/blog.css" rel="stylesheet">
	<link href="../render_base/style_base.css" rel="stylesheet">
	<link href="../render_base/style_chinese.css" rel="stylesheet">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Ma+Shan+Zheng&family=ZCOOL+XiaoWei&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
	<link href="../render_base/sidebar.css" rel="stylesheet">
	<link href="../render_base/common.css" rel="stylesheet">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<main>
<div class='disp-toc-header'>
<h1>LLaMA: Open and Efficient Foundation Language Models</h1><h2 style="font-family: 'ZCOOL XiaoWei', regular;"> 2024-4-10</h2>
</div><p><a href="https://arxiv.org/pdf/2302.13971.pdf">论文原文</a>,</p>
<section>
<h1>intro</h1>
<p>GPT-3 将模型参数扩大到175B, 在此基础上得到了可以用few-shot得到很好结果的实践依据, 这导致一些工作关注继续扩大参数, 认为更多的参数会有更好的效果</p>
<p>但是, 近期工作显示:</p>
<ul>
<li>在固定的计算负载上(训练所需资源):</li>
<p style="margin-bottom: 0em;">更多数据训练的小模型比更少数据训练的大模型要好</p>
<li>在固定的性能上:</li>
<p style="margin-bottom: 0em;">大模型训练到给定性能比小模型要快(小模型需要更多数据)</p>
<p style="margin-bottom: 0em;">小模型推理速度比大模型快</p>
</ul>
<p>本文目标是训练一系列针对特定推理速度下最好的模型, 并得到了LLaMA, 从7B到65B参数的模型, 性能与SOTA模型相当</p>
</section>
<section>
<h1>Approach</h1>
<h2>Pre-training Data</h2>
<p>训练所需所有数据集都是开源的</p>
<img src="../generator/raw_file/blog_markdown/./pic/llmpost/llama/data.png" style="display: block; margin: 0 auto;width: 50%;" alt="err! email me if you need">
<figure><figcaption>Pre-training Data</figcaption></figure><h2>Tokenizer</h2>
<p>使用 byte-pair encoding (BPE) (Each step replaces the most common pair of adjacent data units with a new unit that does not appear in the data, and iterates repeatedly, e. GPT2-4, LLaMA), Sentence-Piece (Encode characters in unicode mode and convert all input, whether it is English, Chinese or other languages, into unicode characters, solving the problem of different encoding methods in multiple languages. Encode spaces as ‘_’ to handle differences in space characters when decoding in different languages)</p>
<p>将所有数字分解为独立个体, 将不认识的UTF-8字符分解为byte</p>
<h2>Architecture</h2>
<ul>
<li>Pre-normalization [GPT-3]</li>
<li>SwiGLU activation function [PaLM]</li>
<li>Rotary Embeddings (RoPE) [GPTNeo]</li>
<li>AdamW optimizer</li>
</ul>
<h2>Efficient implementation</h2>
<ul>
<li>使用xformers高效的causal multi-head attention算法</li>
<li>通过checkpoint保存一部分计算量较大的activation, 减少backward时的重复计算</li>
<li>尽量并行activation的计算和GPUs的通信</li>
</ul>
</section>

	
	<div class="settings-menu">
		<button class="settings-button">Control</button>
		<div class="settings-options">
			<a class="settings-button" href="../index.html">home</a>
			<button id="color_mode_button" class="settings-button" onclick="toggleDarkMode()"> dark</button>
			<a style="color: var(--basic-black);">width <button id="adjustWidth-add" class="settings-inner-button">+</button> / <button id="adjustWidth-sub" class="settings-inner-button">-</button></a>
			<a class="settings-button" href="../blog.html">back to blog</a>
		</div>
	</div>
</main>


<script src="../render_base/sidebar.js"></script> 
<script src="../render_base/common.js"></script> 

</body>
</html>

