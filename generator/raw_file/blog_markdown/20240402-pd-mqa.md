---
title: [MQA] Fast Transformer Decoding: One Write-Head is All You Need
date: 2024-4-2
category: llm paper reading
description:
ps:
language: chinese
type: html
redirect:
---

<a href="http://arxiv.org/abs/1911.02150">论文原文</a>, 本文引用<a href="https://zhuanlan.zhihu.com/p/647130255">1,</a><a href="https://www.bilibili.com/video/BV1ga4y1r7hm/?spm_id_from=333.337.search-card.all.click&vd_source=6823fff4176fbb99da705bde30eb8cea">2</a>


# intro

对于 transformer 中的解码器, 在每一步中需要用到已生成序列的 KV 矩阵, 在有 KV Cache 的情况下, KV 值每次都会被保留下来用于下一步计算, 而在模型很大或者序列长度很长的情况下, 很容易出现KV值无法很好的被缓存的情况

本文提出 Multi-Query Attention, 让多个注意力头之间共享一个KV矩阵, 减少需要保存的 KV 值, 在降低一定精度的情况下获得性能提升



# KV Cache

首先, 缓存 KV 的意义是什么, 为什么不用缓存 Q, 事实上, 在每一步计算中, KV 是对于整个序列的权重信息, 而 Q 矩阵是当前 token 的值, 其中 KV 是逐渐累积的

![70](./pic/llmpost/multiqueryattention/qkv.png)(QKV)

具体过程是(假设已经生成部分序列):

- 解码器得到一个新的结果, 这个结果与之前序列拼接作为下一次 decoder 的输入
- 对新的序列进行位置编码, 对这一次新增的词计算它的 QKV 向量
- 将 KV 向量与之前的 KV Cache 相拼接就得到了接下来 Attention 要用的 KV 矩阵, 同时这个 KV 矩阵代替原先的 KV Cache, Q 矩阵实际上就是2中的 Q 向量(针对 MHA 中的一个 head)

不要与 encoder 传来的 KV 混淆

# Performance

在KV矩阵不断变大的情况下, 很容易就会导致缓存放不下, 这个缓存我理解为除了全局内存以外的显存, 包括共享内存, L1/2 Cache等等(A100 L2 Cache 40MB)

此时就只能去全局内存读数据就会导致访存延时, 通过让一组Q(Multi-Head)共享一个KV矩阵减少访存延时, 此时计算量是不变的, 只是用相同的KV代替原先在各个Head上不同的KV

论文中给出的存算比为(以Batched Multi-Head Attention为例)从$\Theta(\frac{n}{d}+\frac{1}{b})$降低到  $\Theta(\frac{1}{d}+\frac{n}{dh}+\frac{1}{b})$

其中:

- d: 输入长度
- n: MQA调用次数
- h: Head数量
- b: batch
