---
title: GLM-130B: An Open Bilingual Pre-trained Model
date: 2024-4-6
category: llm paper reading
description:
ps:
language: chinese
type: html
redirect:
---

<a href="https://arxiv.org/pdf/2210.02414.pdf">论文原文</a>

# intro

本文将介绍 GLM-130B, 一个有着 130 billion 参数的双语预训练模型, 并分享这样规模的模型的设计思路和训练策略

# design choice

大部分近期(5 Oct 2022)的100B-scale LLMs 比如GPT-3, OPT, PaLM, BLOOM都使用传统的GPT风格, 仅解码器自回归语言模型

GLM-130B 使用 GLM 作为骨干网络, RoPE代替ALiBi作为位置编码, GeLU作为激活函数, 使用DeepNorm类似的Post-LN增强训练稳定性

# pretrain

GLM-130B 的预训练目标不仅包括自监督的GLM空白段自回归, 还有小部分的多任务学习以提高在下游 zero-shot 情况下的表现

- Self-Supervised Blank Infilling(95% tokens)

- Multi-Task Instruction Pre-Training(MIP, 5% tokens)

  T5和ExT5指出预训练多任务学习可能比微调更有效, 并且在预训练阶段进行有助于避免破坏LLMs的通用能力

  建议在GLM-130B的预训练中包括各种指令提示的数据集, 包括语言理解, 生成和信息提取

# parallel strategies

GLM-130B使用 40G A100s, 为了处理巨大的GPU内存需求和应用张量并行导致的整体GPU利用率降低, 我们将流水线并行与数据并行, 张量模型并行相结合得到新的三维并行策略

具体的, 流水线将模型分为几个并行组, 为了最小化流水线引入的bubbles, 使用 PipeDream-Flush来训练具有相对较大全局批次大小(4224)的GLM-130B, 以减少时间和GPU内存浪费. 通过数值和经验检验, 采用了4段张量并行和8段流水线并行

# training stability

考虑到计算资源限制, 必须在不同浮点(FP)格式的效率和稳定性之间做出权衡, 低精度浮点数比如 FP16 能提高计算效率, 但是容易出现上溢和下溢错误, 导致训练崩溃

GLM-130B 使用常见的混合精度: FP16 用于前向反向, FP32 用于优化器和主权重

与 OPT-175B 和 BLOOM-176B 相似, 这种选择导致 GLM-130B 也会面临频繁的损失尖峰, 其中: 

- OPT-175B 试图通过手动跳过数据和调整超参数来解决
- BLOOM-176B选择 embedding norm

GLM-130B 使用 Embedding Layer Gradient Shrink(EGS), 经验表明, 梯度范数可以作为训练崩溃的信息指标:

- 训练崩溃通常滞后于梯度范数尖峰几步
- 这种尖峰通常是由嵌入层的异常梯度引起的

![38,38](./pic/llmpost/glm130b/egs1.png,./pic/llmpost/glm130b/egs2.png)(EGS reduces gradient scale and variance to stabilize LLMs' pre-training)

嵌入层上的梯度收缩可以克服损失尖峰, 从而稳定GLM-130B的训练

# transformer scale up

在使用大量transformer时会出现以下问题:

- 如果使用 Pre-LN, transformer 主分支的值在深层会极大

  GLM-130B通过使用基于DeepNorm的Post-LN, 使得值总是有界的

- 随着模型的扩大, 注意力得分会超过FP16的范围

  PB-Relax 通过在注意力计算中删除偏差项并扣除极值以避免该问题, 但这无助于避免GLM-130B中的不收敛

  BLOOM-176B中, 由于其在NVIDIA Ampere GPU(即A100)上的值范围很广, 因此使用BF16而不是FP16. 然而, BF16比FP16多消耗约15%的运行时GPU内存, 因为它在梯度累积中转换为FP32, 并且它在其他GPU平台上不受支持

  BLOOM-176B的另一个选项是将embedding norm应用于BF16, 但embedding norm会损害模型的zero-shot学习



# GLM-130B Inference on RTX 2080Ti

GLM-130B的一个目标就是在没有效率和效果损失的情况下降低使用100B规模LLM的硬件需求

## INT4 Quantization for RTX 3090s/2080s

实践是将模型权重和激活都量化为INT8

LLM的激活可能包含极端异常值

- OPT-175B和BLOOM-176B中异常值仅影响约0.1%的特征维度, 因此通过对异常维度的矩阵乘法分解来解决
- GLM-130B的激活中存在大约30%的异常值, 这使得上述技术的效率要低得多

GLM-130B将模型权重(大部分是线性层)进行量化(成功达到INT4), 并保持激活层的FP16, 量化模型在运行时会动态转换成FP16, 这引入了较小的计算开销, 但大大减少了存储模型权重所需GPU内存
